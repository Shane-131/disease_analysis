{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified imports with different arrangement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('Data Pipelining, Analysis and Visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('DataPipelineManager')\n",
    "\n",
    "class DataPipelineManager:\n",
    "    \"\"\"\n",
    "    A class to manage data pipelines, handle MySQL database connections,\n",
    "    and provide methods for data manipulation and storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"127.0.0.1\", user: str = \"root\", \n",
    "                 password: str = \"\", database: str = \"data_analytics\",\n",
    "                 port: int = 3306):\n",
    "        \"\"\"\n",
    "        Initialize the DataPipelineManager with a MySQL database connection.\n",
    "        \n",
    "        Args:\n",
    "            host: MySQL server hostname\n",
    "            user: MySQL username\n",
    "            password: MySQL password\n",
    "            database: MySQL database name\n",
    "            port: MySQL server port\n",
    "        \"\"\"\n",
    "        self.connection_params = {\n",
    "            'host': host,\n",
    "            'user': user,\n",
    "            'password': password,\n",
    "            'database': database,\n",
    "            'port': port\n",
    "        }\n",
    "        self.logger = logger\n",
    "        self.conn = self._create_connection()\n",
    "        self.logger.info(f\"Initialized DataPipelineManager with MySQL database at {host}:{port}/{database}\")\n",
    "    \n",
    "    def _create_connection(self) -> pymysql.connections.Connection:\n",
    "        \"\"\"Create and return a MySQL database connection.\"\"\"\n",
    "        try:\n",
    "            # Try to connect to the database\n",
    "            conn = pymysql.connect(**self.connection_params)\n",
    "            return conn\n",
    "        except pymysql.err.OperationalError as e:\n",
    "            error_code = e.args[0]\n",
    "            # Check if the error is due to missing database (error code 1049)\n",
    "            if error_code == 1049:  # Unknown database\n",
    "                try:\n",
    "                    # Connect without specifying database\n",
    "                    temp_params = self.connection_params.copy()\n",
    "                    database_name = temp_params.pop('database')\n",
    "                    temp_conn = pymysql.connect(**temp_params)\n",
    "                    \n",
    "                    # Create the database\n",
    "                    cursor = temp_conn.cursor()\n",
    "                    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "                    cursor.close()\n",
    "                    temp_conn.close()\n",
    "                    \n",
    "                    # Try connecting again\n",
    "                    return pymysql.connect(**self.connection_params)\n",
    "                except pymysql.Error as create_error:\n",
    "                    raise ConnectionError(f\"Failed to create database: {create_error}\")\n",
    "            else:\n",
    "                raise ConnectionError(f\"Database connection error: {e}\")\n",
    "    \n",
    "    def _clean_data(self, data: List[Dict[str, Any]], columns: List[str]) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Clean and prepare data for database insertion.\n",
    "        \n",
    "        Args:\n",
    "            data: List of dictionaries containing data\n",
    "            columns: List of column names to extract from data\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples ready for database insertion\n",
    "        \"\"\"\n",
    "        cleaned_data = []\n",
    "        for row in data:\n",
    "            # Extract only the specified columns\n",
    "            cleaned_row = tuple(row.get(col, None) for col in columns)\n",
    "            cleaned_data.append(cleaned_row)\n",
    "        \n",
    "        self.logger.info(f\"Cleaned {len(cleaned_data)} rows of data\")\n",
    "        return cleaned_data\n",
    "    \n",
    "    def is_connected(self):\n",
    "        \"\"\"Check if the connection is active.\"\"\"\n",
    "        try:\n",
    "            self.conn.ping(reconnect=False)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def save_data(self, table_name: str, columns: List[str], data: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"\n",
    "        Save data to a MySQL database table. Create the table if it doesn't exist.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table to save data to\n",
    "            columns: List of column names\n",
    "            data: List of dictionaries containing data\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "            \n",
    "            cursor = self.conn.cursor()\n",
    "            \n",
    "            # Create table if it doesn't exist\n",
    "            columns_with_types = [f\"`{col}` VARCHAR(255)\" for col in columns]\n",
    "            create_table_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS `{table_name}` (\n",
    "                `id` INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                {', '.join(columns_with_types)},\n",
    "                `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "            \"\"\"\n",
    "            cursor.execute(create_table_query)\n",
    "            \n",
    "            # Clean and prepare data\n",
    "            cleaned_data = self._clean_data(data, columns)\n",
    "            \n",
    "            # Insert data into table\n",
    "            placeholders = ', '.join(['%s' for _ in columns])\n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO `{table_name}` (`{'`, `'.join(columns)}`)\n",
    "            VALUES ({placeholders})\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.executemany(insert_query, cleaned_data)\n",
    "            self.conn.commit()\n",
    "            \n",
    "            self.logger.info(f\"Successfully saved {len(cleaned_data)} rows to table '{table_name}'\")\n",
    "            cursor.close()\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            self.logger.error(f\"Error saving data to table '{table_name}': {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_data(self, \n",
    "                table_name: str, \n",
    "                columns: Optional[List[str]] = None, \n",
    "                limit: Optional[int] = None,\n",
    "                where_clause: Optional[str] = None,\n",
    "                order_by: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query data from a specified table.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table to query\n",
    "            columns: List of columns to retrieve (None for all columns)\n",
    "            limit: Maximum number of rows to return\n",
    "            where_clause: SQL WHERE condition\n",
    "            order_by: SQL ORDER BY clause\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the query results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "            \n",
    "            # Build the SELECT part of the query\n",
    "            cols_str = \"*\" if columns is None else \"`\" + \"`, `\".join(columns) + \"`\"\n",
    "            query = f\"SELECT {cols_str} FROM `{table_name}`\"\n",
    "            \n",
    "            # Add WHERE clause if specified\n",
    "            if where_clause:\n",
    "                query += f\" WHERE {where_clause}\"\n",
    "                \n",
    "            # Add ORDER BY clause if specified\n",
    "            if order_by:\n",
    "                query += f\" ORDER BY {order_by}\"\n",
    "                \n",
    "            # Add LIMIT clause if specified\n",
    "            if limit is not None:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "            \n",
    "            # Execute query using pandas\n",
    "            df = pd.read_sql(query, self.conn)\n",
    "            self.logger.info(f\"Retrieved {len(df)} rows from table '{table_name}'\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving data from table '{table_name}': {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def execute_query(self, query: str, params: tuple = ()) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute a custom SQL query.\n",
    "        \n",
    "        Args:\n",
    "            query: SQL query to execute\n",
    "            params: Parameters for the query\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the query results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "                \n",
    "            # Execute query using pandas\n",
    "            df = pd.read_sql(query, self.conn, params=params)\n",
    "            self.logger.info(f\"Custom query executed successfully, returned {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error executing custom query: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_table_schema(self, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the schema information for a table.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the table schema\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "                \n",
    "            query = f\"DESCRIBE `{table_name}`\"\n",
    "            df = pd.read_sql(query, self.conn)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving schema for table '{table_name}': {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def list_tables(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get a list of all tables in the database.\n",
    "        \n",
    "        Returns:\n",
    "            List of table names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "                \n",
    "            cursor = self.conn.cursor()\n",
    "            cursor.execute(\"SHOW TABLES\")\n",
    "            tables = [row[0] for row in cursor.fetchall()]\n",
    "            cursor.close()\n",
    "            return tables\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing tables: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_index(self, table_name: str, columns: List[str], index_name: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Create an index on specified columns.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table\n",
    "            columns: List of columns to include in the index\n",
    "            index_name: Name of the index (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        if not index_name:\n",
    "            index_name = f\"idx_{table_name}_{'_'.join(columns)}\"\n",
    "            \n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "                \n",
    "            cursor = self.conn.cursor()\n",
    "            query = f\"CREATE INDEX `{index_name}` ON `{table_name}` (`{'`, `'.join(columns)}`)\"\n",
    "            cursor.execute(query)\n",
    "            self.conn.commit()\n",
    "            cursor.close()\n",
    "            self.logger.info(f\"Created index '{index_name}' on table '{table_name}'\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            self.logger.error(f\"Error creating index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def perform_data_transformation(self, df: pd.DataFrame, transformations: Dict[str, callable]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply a series of transformations to a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            transformations: Dictionary mapping column names to transformation functions\n",
    "            \n",
    "        Returns:\n",
    "            Transformed DataFrame\n",
    "        \"\"\"\n",
    "        transformed_df = df.copy()\n",
    "        \n",
    "        for column, transform_func in transformations.items():\n",
    "            if column in transformed_df.columns:\n",
    "                transformed_df[column] = transformed_df[column].apply(transform_func)\n",
    "                \n",
    "        return transformed_df\n",
    "    \n",
    "    def execute_transaction(self, queries: List[Tuple[str, tuple]]) -> bool:\n",
    "        \"\"\"\n",
    "        Execute multiple queries as a single transaction.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of (query, params) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "                \n",
    "            cursor = self.conn.cursor()\n",
    "            for query, params in queries:\n",
    "                cursor.execute(query, params)\n",
    "            self.conn.commit()\n",
    "            cursor.close()\n",
    "            self.logger.info(f\"Transaction with {len(queries)} queries executed successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            self.logger.error(f\"Transaction failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def backup_table(self, table_name: str, backup_suffix: str = \"_backup\") -> bool:\n",
    "        \"\"\"\n",
    "        Create a backup of a table.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table to backup\n",
    "            backup_suffix: Suffix to append to the original table name\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        backup_table = f\"{table_name}{backup_suffix}\"\n",
    "        try:\n",
    "            # Reconnect if connection is closed\n",
    "            if not self.is_connected():\n",
    "                self.conn = self._create_connection()\n",
    "                \n",
    "            cursor = self.conn.cursor()\n",
    "            \n",
    "            # Drop the backup table if it exists\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS `{backup_table}`\")\n",
    "            \n",
    "            # Create the backup table\n",
    "            cursor.execute(f\"CREATE TABLE `{backup_table}` LIKE `{table_name}`\")\n",
    "            cursor.execute(f\"INSERT INTO `{backup_table}` SELECT * FROM `{table_name}`\")\n",
    "            self.conn.commit()\n",
    "            cursor.close()\n",
    "            self.logger.info(f\"Created backup of table '{table_name}' as '{backup_table}'\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            self.logger.error(f\"Error backing up table: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.logger.info(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:41:27,571 - DataPipelineManager - INFO - Initialized DataPipelineManager with MySQL database at 127.0.0.1:3306/data_analytics\n"
     ]
    }
   ],
   "source": [
    "# load database credentials\n",
    "load_dotenv()\n",
    "\n",
    "HOST=os.getenv(\"HOST\")\n",
    "USER=os.getenv(\"USER\")\n",
    "PASSWORD=os.getenv(\"PASSWORD\")\n",
    "DATABASE=os.getenv(\"DATABASE\")\n",
    "PORT=os.getenv(\"PORT\")\n",
    "\n",
    "db = DataPipelineManager(host=HOST, user=USER, password=PASSWORD, database=DATABASE, port=int(PORT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onfon/Desktop/isaac/freelance/creekpen/datasets\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m data_dict = retrieve_csv_data()\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Assign individual datasets to variables\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m disease_dataset = data_dict[\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[32m     35\u001b[39m aids_trials = data_dict[\u001b[38;5;28mlist\u001b[39m(data_dict.keys())[\u001b[32m1\u001b[39m]]\n\u001b[32m     36\u001b[39m symptom_data = data_dict[\u001b[38;5;28mlist\u001b[39m(data_dict.keys())[\u001b[32m2\u001b[39m]]\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "def retrieve_csv_data() -> dict:\n",
    "    \"\"\"\n",
    "    Locate and load CSV files from the 'datasets' directory\n",
    "    Returns: Dictionary with dataset names as keys and DataFrames as values\n",
    "    \"\"\"\n",
    "    dataset_collection = {}\n",
    "    \n",
    "    # Navigate directory structure\n",
    "    current_dir = os.getcwd()\n",
    "    # upper_dir = os.path.dirname(current_dir)\n",
    "    data_folder = os.path.join(current_dir, 'datasets')\n",
    "    print(data_folder)\n",
    "\n",
    "    \n",
    "    # Find all CSV files\n",
    "    csv_file_paths = []\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file_paths.append(f\"datasets/{file}\")\n",
    "    \n",
    "    # Process each dataset\n",
    "    for idx, filepath in enumerate(csv_file_paths):\n",
    "        file_data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), filepath))\n",
    "        file_name = filepath.split(\"/\")[1].split(\".\")[0]\n",
    "        print(f\"{idx+1}: {file_name}\")\n",
    "        dataset_collection[file_name] = file_data\n",
    "    \n",
    "    return dataset_collection\n",
    "\n",
    "# Load datasets into dictionary\n",
    "data_dict = retrieve_csv_data()\n",
    "\n",
    "# Assign individual datasets to variables\n",
    "disease_dataset = data_dict[list(data_dict.keys())[0]]\n",
    "aids_trials = data_dict[list(data_dict.keys())[1]]\n",
    "symptom_data = data_dict[list(data_dict.keys())[2]]\n",
    "disease_info = data_dict[list(data_dict.keys())[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data: pd.DataFrame, target_col: str, excluded_patterns: list=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare dataset for analysis by removing unwanted columns, duplicates,\n",
    "    and handling missing values\n",
    "    \n",
    "    Args:\n",
    "        data: Input DataFrame\n",
    "        target_col: Target column name\n",
    "        excluded_patterns: List of patterns to exclude in column names\n",
    "        \n",
    "    Returns: Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    if excluded_patterns is None:\n",
    "        excluded_patterns = []\n",
    "    \n",
    "    # Remove columns matching exclusion patterns\n",
    "    filtered_df = data.drop([col for col in data.columns if any(pattern in col for pattern in excluded_patterns)], axis=1)\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    filtered_df = filtered_df.loc[~filtered_df.duplicated()]\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = filtered_df.isnull().sum()\n",
    "    if missing_count.sum() > 0:\n",
    "        print(missing_count)\n",
    "    else:\n",
    "        print(\"No missing values detected\")\n",
    "    \n",
    "    # Split features and target\n",
    "    features = filtered_df.drop(target_col, axis=1)\n",
    "    target = filtered_df[target_col]\n",
    "    \n",
    "    # Create train-test split\n",
    "    features_train, features_test, target_train, target_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    feature_scaler = StandardScaler()\n",
    "    features_train_scaled = feature_scaler.fit_transform(features_train)\n",
    "    features_test_scaled = feature_scaler.transform(features_test)\n",
    "    \n",
    "    # Restore column names after scaling\n",
    "    features_train_scaled = pd.DataFrame(features_train_scaled, columns=features_train.columns)\n",
    "    features_test_scaled = pd.DataFrame(features_test_scaled, columns=features_test.columns)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare first dataset\n",
    "clean_disease_data = prepare_dataset(dataset=disease_dataset, target_col='disease', excluded_patterns=['Unnamed'])\n",
    "\n",
    "# Generate disease distribution visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "disease_distribution = clean_disease_data['disease'].value_counts()\n",
    "sns.barplot(x=disease_distribution.values, y=disease_distribution.index)\n",
    "plt.title('Distribution of Disease Cases')\n",
    "plt.xlabel('Number of Cases')\n",
    "plt.ylabel('Disease Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze symptom correlations\n",
    "symptom_features = clean_disease_data.columns[:-1]\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(clean_disease_data[symptom_features].corr(), cmap='viridis', center=0, annot=False)\n",
    "plt.title('Symptom Correlation Analysis')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create symptom distribution boxplot by disease\n",
    "plt.figure(figsize=(15, 6))\n",
    "reshaped_data = clean_disease_data.melt(id_vars=['disease'], value_vars=symptom_features)\n",
    "sns.boxplot(x='disease', y='value', data=reshaped_data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Symptom Intensity Across Disease Categories')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze top symptoms per disease\n",
    "leading_symptoms = clean_disease_data[symptom_features].mean().sort_values(ascending=False)[:10].index\n",
    "disease_symptom_profile = pd.DataFrame()\n",
    "for disease_type in disease_distribution.index:\n",
    "    disease_subset = clean_disease_data[clean_disease_data['disease'] == disease_type][leading_symptoms].mean()\n",
    "    disease_symptom_profile[disease_type] = disease_subset\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "disease_symptom_profile.T.plot(kind='bar', stacked=True)\n",
    "plt.title('Main Symptom Distribution by Disease')\n",
    "plt.xlabel('Disease Category')\n",
    "plt.ylabel('Symptom Frequency')\n",
    "plt.legend(title='Symptoms', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare symptom dataset\n",
    "clean_symptom_data = prepare_dataset(dataset=symptom_data, target_col='prognosis')\n",
    "\n",
    "# Visualize prognosis distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "prognosis_distribution = clean_symptom_data['prognosis'].value_counts()\n",
    "sns.barplot(x=prognosis_distribution.index, y=prognosis_distribution.values)\n",
    "plt.title('Prognosis Case Distribution')\n",
    "plt.xlabel('Prognosis Category')\n",
    "plt.ylabel('Case Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Symptom importance analysis using chi-square\n",
    "symptom_variables = clean_symptom_data.columns[:-1]\n",
    "importance_scores = {}\n",
    "\n",
    "for symptom in symptom_variables:\n",
    "    contingency = pd.crosstab(clean_symptom_data[symptom], clean_symptom_data['prognosis'])\n",
    "    chi2_val, p_value, _, _ = stats.chi2_contingency(contingency)\n",
    "    importance_scores[symptom] = chi2_val\n",
    "\n",
    "# Prepare importance dataframe\n",
    "symptom_importance = pd.DataFrame(list(importance_scores.items()),\n",
    "                                  columns=['Symptom_Name', 'Statistical_Significance'])\n",
    "symptom_importance = symptom_importance.sort_values('Statistical_Significance', ascending=False)\n",
    "\n",
    "# Plot top significant symptoms\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=symptom_importance.head(20), x='Statistical_Significance', y='Symptom_Name')\n",
    "plt.title('Most Significant Symptoms for Prognosis Assessment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create symptom co-occurrence visualization\n",
    "plt.figure(figsize=(15, 15))\n",
    "correlation_data = clean_symptom_data[symptom_variables].corr()\n",
    "upper_mask = np.triu(np.ones_like(correlation_data, dtype=bool))\n",
    "sns.heatmap(correlation_data, mask=upper_mask, center=0, cmap='RdBu_r',\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Symptom Co-occurrence Pattern Analysis')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prognosis-specific symptom analysis\n",
    "top_prognoses = prognosis_distribution.head().index\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "flattened_axes = axes.ravel()\n",
    "\n",
    "for i, prognosis_type in enumerate(top_prognoses):\n",
    "    prognosis_subset = clean_symptom_data[clean_symptom_data['prognosis'] == prognosis_type][symptom_variables].mean()\n",
    "    primary_symptoms = prognosis_subset.sort_values(ascending=False)[:10]\n",
    "    sns.barplot(x=primary_symptoms.values, y=primary_symptoms.index, ax=flattened_axes[i])\n",
    "    flattened_axes[i].set_title(f'Primary Symptoms for {prognosis_type}')\n",
    "    flattened_axes[i].set_xlabel('Prevalence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA analysis for symptom clustering\n",
    "feature_normalizer = StandardScaler()\n",
    "normalized_symptoms = feature_normalizer.fit_transform(clean_symptom_data[symptom_variables])\n",
    "normalized_symptoms = pd.DataFrame(normalized_symptoms, columns=symptom_variables)\n",
    "normalized_symptoms['prognosis'] = clean_symptom_data['prognosis']\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "reducer = PCA(n_components=2)\n",
    "reduced_data = reducer.fit_transform(clean_symptom_data[symptom_variables])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter_plot = plt.scatter(reduced_data[:, 0], reduced_data[:, 1],\n",
    "                          c=pd.factorize(clean_symptom_data['prognosis'])[0],\n",
    "                          alpha=0.6)\n",
    "plt.title('Symptom Pattern Clusters by Prognosis (Dimensionality Reduction)')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.legend(handles=scatter_plot.legend_elements()[0],\n",
    "          labels=list(clean_symptom_data['prognosis'].unique()),\n",
    "          title='Prognosis',\n",
    "          bbox_to_anchor=(1.05, 1),\n",
    "          loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "analysis_summary = {\n",
    "    'total_prognosis_categories': len(prognosis_distribution),\n",
    "    'predominant_prognosis': prognosis_distribution.index[0],\n",
    "    'key_indicator_symptom': symptom_importance.iloc[0]['Symptom_Name'],\n",
    "    'total_tracked_symptoms': len(symptom_variables)\n",
    "}\n",
    "\n",
    "print(\"\\nKey Analysis Findings:\")\n",
    "for metric, value in analysis_summary.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and analyze AIDS clinical trial data\n",
    "processed_aids_data = prepare_dataset(dataset=aids_trials, target_col='label', excluded_patterns=['Unnamed'])\n",
    "\n",
    "# AIDS case distribution visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=processed_aids_data, x='label')\n",
    "plt.title('AIDS Diagnosis Distribution')\n",
    "plt.xlabel('AIDS Status (0: Negative, 1: Positive)')\n",
    "plt.ylabel('Patient Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demographic analysis by AIDS status\n",
    "fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Age distribution\n",
    "sns.boxplot(data=processed_aids_data, x='label', y='age', ax=axis1)\n",
    "axis1.set_title('Patient Age Distribution by AIDS Status')\n",
    "axis1.set_xlabel('AIDS Status')\n",
    "axis1.set_ylabel('Age (years)')\n",
    "\n",
    "# Weight distribution\n",
    "sns.boxplot(data=processed_aids_data, x='label', y='wtkg', ax=axis2)\n",
    "axis2.set_title('Patient Weight Distribution by AIDS Status')\n",
    "axis2.set_xlabel('AIDS Status')\n",
    "axis2.set_ylabel('Weight (kg)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# CD4 count analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=processed_aids_data, x='cd40', y='cd420', hue='label', alpha=0.6)\n",
    "plt.title('CD4 Count Progression: Initial vs Week 20')\n",
    "plt.xlabel('Baseline CD4 Count')\n",
    "plt.ylabel('Week 20 CD4 Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk factor analysis\n",
    "risk_categories = ['homo', 'drugs']\n",
    "fig, plot_axes = plt.subplots(1, len(risk_categories), figsize=(15, 6))\n",
    "\n",
    "for idx, factor in enumerate(risk_categories):\n",
    "    sns.heatmap(pd.crosstab(processed_aids_data[factor], processed_aids_data['label'], normalize='index'),\n",
    "               annot=True, fmt='.2%', cmap='Oranges', ax=plot_axes[idx])\n",
    "    plot_axes[idx].set_title(f'{factor.capitalize()} Status vs AIDS Diagnosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Treatment effect analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "treatment_outcomes = processed_aids_data.groupby(['treat', 'label']).size().unstack()\n",
    "treatment_outcomes.plot(kind='bar', stacked=True)\n",
    "plt.title('AIDS Status Distribution Across Treatment Groups')\n",
    "plt.xlabel('Treatment Group')\n",
    "plt.ylabel('Patient Count')\n",
    "plt.legend(title='AIDS Status')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis of clinical parameters\n",
    "numeric_variables = ['age', 'wtkg', 'cd40', 'cd420', 'cd80', 'cd820', 'karnof']\n",
    "plt.figure(figsize=(12, 10))\n",
    "parameter_correlations = processed_aids_data[numeric_variables + ['label']].corr()\n",
    "sns.heatmap(parameter_correlations, annot=True, cmap='RdBu', center=0)\n",
    "plt.title('Clinical Parameter Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Symptom severity analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=processed_aids_data, x='symptom', hue='label')\n",
    "plt.title('Symptom Severity by AIDS Status')\n",
    "plt.xlabel('Symptom Level')\n",
    "plt.ylabel('Patient Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Immune system ratio analysis\n",
    "processed_aids_data['baseline_cd4_cd8_ratio'] = processed_aids_data['cd40'] / processed_aids_data['cd80']\n",
    "processed_aids_data['followup_cd4_cd8_ratio'] = processed_aids_data['cd420'] / processed_aids_data['cd820']\n",
    "\n",
    "fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.boxplot(data=processed_aids_data, x='label', y='baseline_cd4_cd8_ratio', ax=axis1)\n",
    "axis1.set_title('Baseline CD4/CD8 Ratio by AIDS Status')\n",
    "axis1.set_xlabel('AIDS Status')\n",
    "axis1.set_ylabel('CD4/CD8 Ratio')\n",
    "\n",
    "sns.boxplot(data=processed_aids_data, x='label', y='followup_cd4_cd8_ratio', ax=axis2)\n",
    "axis2.set_title('Week 20 CD4/CD8 Ratio by AIDS Status')\n",
    "axis2.set_xlabel('AIDS Status')\n",
    "axis2.set_ylabel('CD4/CD8 Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance testing\n",
    "print(\"\\nKey Statistical Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Age difference test\n",
    "age_tstat, age_pval = stats.ttest_ind(processed_aids_data[processed_aids_data['label']==1]['age'],\n",
    "                                    processed_aids_data[processed_aids_data['label']==0]['age'])\n",
    "print(f\"Age difference significance (t-test p-value): {age_pval:.4f}\")\n",
    "\n",
    "# Treatment effect test\n",
    "treat_chi2, treat_pval = stats.chi2_contingency(pd.crosstab(processed_aids_data['treat'], \n",
    "                                                          processed_aids_data['label']))[0:2]\n",
    "print(f\"Treatment effect significance (chi-square p-value): {treat_pval:.4f}\")\n",
    "\n",
    "# CD4 count change\n",
    "processed_aids_data['cd4_delta'] = processed_aids_data['cd420'] - processed_aids_data['cd40']\n",
    "cd4_tstat, cd4_pval = stats.ttest_ind(processed_aids_data[processed_aids_data['label']==1]['cd4_delta'],\n",
    "                                    processed_aids_data[processed_aids_data['label']==0]['cd4_delta'])\n",
    "print(f\"CD4 count change significance (t-test p-value): {cd4_pval:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
